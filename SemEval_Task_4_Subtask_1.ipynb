{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Set up environment"
      ],
      "metadata": {
        "id": "TWpz0QHOJJmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1CWHTvioOPjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA0XwJnO3tHf"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabulate"
      ],
      "metadata": {
        "id": "ue1peF5W_UcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y94tBvHb2ZxP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from tabulate import tabulate\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hierarchical Classification Metric Implementation"
      ],
      "metadata": {
        "id": "QAE-jTL_iK8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-hierarchical-classification"
      ],
      "metadata": {
        "id": "TtuIxBtQh4uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "import json\n",
        "import logging.handlers\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from networkx import DiGraph, relabel_nodes, all_pairs_shortest_path_length\n",
        "from sklearn_hierarchical_classification.constants import ROOT\n",
        "from sklearn_hierarchical_classification.metrics import h_fbeta_score, h_recall_score, h_precision_score, \\\n",
        "    fill_ancestors, multi_labeled"
      ],
      "metadata": {
        "id": "SAA3c8BohdvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = DiGraph()\n",
        "G.add_edge(ROOT, \"Logos\")\n",
        "G.add_edge(\"Logos\", \"Repetition\")\n",
        "G.add_edge(\"Logos\", \"Obfuscation, Intentional vagueness, Confusion\")\n",
        "G.add_edge(\"Logos\", \"Reasoning\")\n",
        "G.add_edge(\"Logos\", \"Justification\")\n",
        "G.add_edge('Justification', \"Slogans\")\n",
        "G.add_edge('Justification', \"Bandwagon\")\n",
        "G.add_edge('Justification', \"Appeal to authority\")\n",
        "G.add_edge('Justification', \"Flag-waving\")\n",
        "G.add_edge('Justification', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Reasoning', \"Simplification\")\n",
        "G.add_edge('Simplification', \"Causal Oversimplification\")\n",
        "G.add_edge('Simplification', \"Black-and-white Fallacy/Dictatorship\")\n",
        "G.add_edge('Simplification', \"Thought-terminating cliché\")\n",
        "G.add_edge('Reasoning', \"Distraction\")\n",
        "G.add_edge('Distraction', \"Misrepresentation of Someone's Position (Straw Man)\")\n",
        "G.add_edge('Distraction', \"Presenting Irrelevant Data (Red Herring)\")\n",
        "G.add_edge('Distraction', \"Whataboutism\")\n",
        "G.add_edge(ROOT, \"Ethos\")\n",
        "G.add_edge('Ethos', \"Appeal to authority\")\n",
        "G.add_edge('Ethos', \"Glittering generalities (Virtue)\")\n",
        "G.add_edge('Ethos', \"Bandwagon\")\n",
        "G.add_edge('Ethos', \"Ad Hominem\")\n",
        "G.add_edge('Ethos', \"Transfer\")\n",
        "G.add_edge('Ad Hominem', \"Doubt\")\n",
        "G.add_edge('Ad Hominem', \"Name calling/Labeling\")\n",
        "G.add_edge('Ad Hominem', \"Smears\")\n",
        "G.add_edge('Ad Hominem', \"Reductio ad hitlerum\")\n",
        "G.add_edge('Ad Hominem', \"Whataboutism\")\n",
        "G.add_edge(ROOT, \"Pathos\")\n",
        "G.add_edge('Pathos', \"Exaggeration/Minimisation\")\n",
        "G.add_edge('Pathos', \"Loaded Language\")\n",
        "G.add_edge('Pathos', \"Appeal to (Strong) Emotions\")\n",
        "G.add_edge('Pathos', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Pathos', \"Flag-waving\")\n",
        "G.add_edge('Pathos', \"Transfer\")"
      ],
      "metadata": {
        "id": "x1qqLe6eiIzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _read_gold_and_pred(pred_fpath):\n",
        "  \"\"\"\n",
        "  Read gold and predicted data.\n",
        "  :param pred_fpath: a json file with predictions,\n",
        "  :param gold_fpath: the original annotated gold file.\n",
        "  :return: {id:pred_labels} dict; {id:gold_labels} dict\n",
        "  \"\"\"\n",
        "\n",
        "  gold_labels = {}\n",
        "  with open('/content/drive/MyDrive/SemEval/data/subtask1/validation.json', encoding='utf-8') as gold_f:\n",
        "    gold = json.load(gold_f)\n",
        "    #print(len(gold))\n",
        "    for obj in gold:\n",
        "      gold_labels[obj['id']] = obj['labels']\n",
        "\n",
        "\n",
        "  pred_labels = {}\n",
        "  with open(pred_fpath, encoding='utf-8') as pred_f:\n",
        "    pred = json.load(pred_f)\n",
        "    #print(len(pred))\n",
        "    for obj in pred:\n",
        "      pred_labels[obj['id']] = obj['labels']\n",
        "\n",
        "  if set(gold_labels.keys()) != set(pred_labels.keys()):\n",
        "      print('There are either missing or added examples to the prediction file. Make sure you only have the gold examples in the prediction file.')\n",
        "\n",
        "  return pred_labels, gold_labels\n"
      ],
      "metadata": {
        "id": "2V5PrL7nl-Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_classes_from_graph(graph):\n",
        "    return [\n",
        "        node\n",
        "        for node in graph.nodes\n",
        "        if node != ROOT\n",
        "        ]\n",
        "\n",
        "def _h_fbeta_score(y_true, y_pred, class_hierarchy, beta=1., root=ROOT):\n",
        "    hP = _h_precision_score(y_true, y_pred, class_hierarchy, root=root)\n",
        "    hR = _h_recall_score(y_true, y_pred, class_hierarchy, root=root)\n",
        "    if hP == 0 and hR == 0:\n",
        "      return 0\n",
        "    return (1. + beta ** 2.) * hP * hR / (beta ** 2. * hP + hR)\n",
        "\n",
        "def _fill_ancestors(y, graph, root, copy=True):\n",
        "    y_ = y.copy() if copy else y\n",
        "    paths = all_pairs_shortest_path_length(graph.reverse(copy=False))\n",
        "    for target, distances in paths:\n",
        "        if target == root:\n",
        "            continue\n",
        "        #print(target)\n",
        "        ix_rows = np.where(y[:, target] > 0)[0]\n",
        "        ancestors = list(filter(lambda x: x != ROOT,distances.keys()))\n",
        "        y_[tuple(np.meshgrid(ix_rows, ancestors))] = 1\n",
        "    graph.reverse(copy=False)\n",
        "    return y_\n",
        "def _h_recall_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
        "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
        "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
        "\n",
        "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
        "\n",
        "    true_positives = len(ix[0])\n",
        "    all_positives = np.count_nonzero(y_true_)\n",
        "\n",
        "    if all_positives == 0:\n",
        "      return 0\n",
        "\n",
        "    return true_positives / all_positives\n",
        "\n",
        "def _h_precision_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
        "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
        "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
        "\n",
        "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
        "\n",
        "    true_positives = len(ix[0])\n",
        "    all_results = np.count_nonzero(y_pred_)\n",
        "\n",
        "    if all_results == 0:\n",
        "      return 0\n",
        "\n",
        "    return true_positives / all_results"
      ],
      "metadata": {
        "id": "F5wT_4ZqiWU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(graph, pred_file):\n",
        "\n",
        "  pred_labels, gold_labels = _read_gold_and_pred(pred_file)\n",
        "  gold = []\n",
        "  pred = []\n",
        "\n",
        "  for id in gold_labels:\n",
        "        gold.append(gold_labels[id])\n",
        "        pred.append(pred_labels[id])\n",
        "\n",
        "  with multi_labeled(gold, pred, G) as (gold_, pred_, graph_):\n",
        "        return  _h_precision_score(gold_, pred_,graph_), _h_recall_score(gold_, pred_,graph_), _h_fbeta_score(gold_, pred_,graph_)\n"
      ],
      "metadata": {
        "id": "aweU2qk3isNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "DHLabuvJiSAA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzg0A049wi_u"
      },
      "outputs": [],
      "source": [
        "def read_classes(file_path):\n",
        "  CLASSES = []\n",
        "  with open(file_path) as f:\n",
        "    for label in f.readlines():\n",
        "      label = label.strip()\n",
        "      if label:\n",
        "        CLASSES.append(label)\n",
        "  return CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L9OYh-n1jGC"
      },
      "outputs": [],
      "source": [
        "labelSet = read_classes('label_file_path')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvI2XrEv1obo"
      },
      "outputs": [],
      "source": [
        "print(len(labelSet))\n",
        "print(labelSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spqcVpxC3mEu"
      },
      "outputs": [],
      "source": [
        "all_labels = ['Repetition', 'Obfuscation, Intentional vagueness, Confusion', 'Slogans', 'Bandwagon', 'Appeal to authority', 'Flag-waving', 'Appeal to fear/prejudice','Causal Oversimplification', 'Black-and-white Fallacy/Dictatorship', 'Thought-terminating cliché', \"Misrepresentation of Someone's Position (Straw Man)\", 'Presenting Irrelevant Data (Red Herring)', 'Whataboutism', 'Glittering generalities (Virtue)', 'Doubt', 'Name calling/Labeling', 'Smears', 'Reductio ad hitlerum', 'Exaggeration/Minimisation', 'Loaded Language']\n",
        "print(len(all_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "id": "AjXnV-jC1mzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67y44p0g4ObL"
      },
      "outputs": [],
      "source": [
        "# Load the data from JSON files\n",
        "with open('train_file_path', 'r') as train_file:\n",
        "    train_data = json.load(train_file)\n",
        "\n",
        "with open('valid_file_path', 'r') as valid_file:\n",
        "    valid_data = json.load(valid_file)\n",
        "\n",
        "valid_ids = [instance['id'] for instance in valid_data]\n",
        "\n",
        "with open('test_file_path', 'r') as test_file:\n",
        "    test_data = json.load(test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFVN-sDiyrU5"
      },
      "outputs": [],
      "source": [
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4rbEWGa3ZbX"
      },
      "outputs": [],
      "source": [
        "# Convert labels to one-hot encoded tensors\n",
        "def one_hot_encode_labels(labels, label_list):\n",
        "    label_dict = {label: i for i, label in enumerate(label_list)}\n",
        "    one_hot = torch.zeros(len(label_list))\n",
        "    for label in labels:\n",
        "        one_hot[label_dict[label]] = 1\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMvDPrFV3aLA"
      },
      "outputs": [],
      "source": [
        "# Tokenize and encode the text data\n",
        "def tokenize_and_encode_text(instance, model_name, tokenizer):\n",
        "    encoding = tokenizer(instance['text'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MicxxrTL12LN"
      },
      "outputs": [],
      "source": [
        "def prepare_data(train_data, valid_data, test_data, model_name):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  # Tokenize and encode all instances in the training and validation datasets\n",
        "  train_encodings = [tokenize_and_encode_text(instance,model_name,tokenizer) for instance in train_data]\n",
        "  valid_encodings = [tokenize_and_encode_text(instance,model_name,tokenizer) for instance in valid_data]\n",
        "  test_encodings = [tokenize_and_encode_text(instance,model_name,tokenizer) for instance in test_data]\n",
        "\n",
        "  # One-hot encode labels for all instances\n",
        "  train_labels = [one_hot_encode_labels(instance['labels'], all_labels) for instance in train_data]\n",
        "  valid_labels = [one_hot_encode_labels(instance['labels'], all_labels) for instance in valid_data]\n",
        "\n",
        "  # Convert the lists of encodings and labels to PyTorch tensors\n",
        "  train_input_ids = torch.cat([encoding['input_ids'] for encoding in train_encodings], dim=0)\n",
        "  train_attention_mask = torch.cat([encoding['attention_mask'] for encoding in train_encodings], dim=0)\n",
        "  train_labels = torch.stack(train_labels)\n",
        "\n",
        "  valid_input_ids = torch.cat([encoding['input_ids'] for encoding in valid_encodings], dim=0)\n",
        "  valid_attention_mask = torch.cat([encoding['attention_mask'] for encoding in valid_encodings], dim=0)\n",
        "  valid_labels = torch.stack(valid_labels)\n",
        "\n",
        "  test_input_ids = torch.cat([encoding['input_ids'] for encoding in test_encodings], dim=0)\n",
        "  test_attention_mask = torch.cat([encoding['attention_mask'] for encoding in test_encodings], dim=0)\n",
        "\n",
        "\n",
        "  # Create a list of 'id' strings\n",
        "  test_ids = [instance['id'] for instance in test_data]\n",
        "  print(\"Test shape: \",test_input_ids.shape)\n",
        "\n",
        "\n",
        "  train_input_ids = train_input_ids.to(device)\n",
        "  train_attention_mask = train_attention_mask.to(device)\n",
        "  train_labels = train_labels.to(device)\n",
        "\n",
        "  valid_input_ids = valid_input_ids.to(device)\n",
        "  valid_attention_mask = valid_attention_mask.to(device)\n",
        "  valid_labels = valid_labels.to(device)\n",
        "\n",
        "  test_input_ids = test_input_ids.to(device)\n",
        "  test_attention_mask = test_attention_mask.to(device)\n",
        "\n",
        "\n",
        "  return train_input_ids, train_attention_mask, train_labels, valid_input_ids, valid_attention_mask, valid_labels, test_input_ids, test_attention_mask, test_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9y6IC8z9UPA"
      },
      "outputs": [],
      "source": [
        "def train_model(classifier_model, train_dataloader):\n",
        "  # Define loss function and optimizer\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam(classifier_model.parameters(), lr=2e-5)\n",
        "\n",
        "  # Training loop\n",
        "  num_epochs = 10 # You can adjust the number of epochs\n",
        "  for epoch in range(num_epochs):\n",
        "      classifier_model.train()\n",
        "      total_loss = 0\n",
        "      for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
        "          input_ids, attention_mask, labels = batch\n",
        "          # Move input tensors to the same device as the model\n",
        "          input_ids = input_ids.to(device)\n",
        "          attention_mask = attention_mask.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          logits = classifier_model(input_ids, attention_mask)\n",
        "          loss = criterion(logits, labels.float())\n",
        "          total_loss+=loss.item()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      average_loss = total_loss / len(train_dataloader)\n",
        "      print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}')\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(classifier_model.state_dict(), 'custom_classifier_model.pth')\n",
        "  return classifier_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtDBLi01_KFZ"
      },
      "outputs": [],
      "source": [
        "def validate_model(classifier_model, valid_dataloader, model):\n",
        "\n",
        "  # Initialize lists to store true labels and predicted labels\n",
        "  true_labels = []\n",
        "  predicted_labels = []\n",
        "\n",
        "  # Iterate over the validation dataset\n",
        "  with torch.no_grad():\n",
        "      for batch in valid_dataloader:  # Assuming you have set up a DataLoader for the validation dataset\n",
        "          input_ids, attention_mask, labels = batch\n",
        "          logits = classifier_model(input_ids, attention_mask)\n",
        "\n",
        "          # Apply sigmoid activation to the logits to get predicted probabilities\n",
        "          predicted_probs = torch.sigmoid(logits)\n",
        "\n",
        "          true_labels.extend(labels.cpu().numpy())\n",
        "          predicted_labels.extend(predicted_probs.cpu().numpy())\n",
        "\n",
        "  # Convert the lists to NumPy arrays\n",
        "  p_labels = predicted_labels\n",
        "  t_labels = true_labels\n",
        "  true_labels = np.array(true_labels)\n",
        "  predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "  # Apply a threshold to predicted probabilities to determine the predicted labels\n",
        "  threshold = 0.25  # You can adjust this threshold based on your requirements\n",
        "  class_threshold = [0.4, 0.3, 0.2, 0.01, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1, 0.01, 0.01, 0.2, 0.1, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1]\n",
        "  predicted_labels = (predicted_labels > threshold).astype(int)\n",
        "\n",
        "  # Prepare the results in the desired format (id and labels) and save to a JSON file\n",
        "  results = [{'id': valid_id, 'labels': [all_labels[i] for i, value in enumerate(instance) if value == 1]} for valid_id, instance in zip(valid_ids, predicted_labels)]\n",
        "\n",
        "  output_file_path = \"path_to_val_results\" + model + \"_val_results.json\"\n",
        "  with open(output_file_path, 'w') as output_file:\n",
        "      json.dump(results, output_file, indent=2)\n",
        "\n",
        "\n",
        "  precision, recall, f1, support = precision_recall_fscore_support(true_labels, predicted_labels, average=None, zero_division=0)\n",
        "\n",
        "  precision_h, recall_h, f1_h = calculate_metrics(G, output_file_path)\n",
        "  print(\"HP\",precision_h)\n",
        "  print(\"HR\",recall_h)\n",
        "  print(\"HF\",f1_h)\n",
        "\n",
        "\n",
        "  # Create a list to store the results\n",
        "  results = []\n",
        "\n",
        "  # Populate the results list with class-wise metrics\n",
        "  for i, class_name in enumerate(all_labels):\n",
        "   results.append([class_name, f\"{f1[i]:.2f}\", f\"{precision[i]:.2f}\", f\"{recall[i]:.2f}\"])\n",
        "\n",
        "\n",
        "  # Print the results in a table format\n",
        "  headers = [\"Class\", \"F1-Score\", \"Precision\", \"Recall\", \"Support\"]\n",
        "  print(tabulate(results, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "  return p_labels, t_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEKhbP2j_Vcn"
      },
      "outputs": [],
      "source": [
        "def test_model(classifier_model, test_data, model):\n",
        "\n",
        "  cnt = 0\n",
        "\n",
        "  # Make predictions on the test dataset\n",
        "  predicted_labels = []\n",
        "  p_test_ids = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch in test_dataloader:\n",
        "          input_ids, attention_mask= batch\n",
        "          input_ids = input_ids.to(device)\n",
        "          attention_mask = attention_mask.to(device)\n",
        "          logits = classifier_model(input_ids, attention_mask)\n",
        "\n",
        "          # Apply sigmoid activation to the logits to get predicted probabilities\n",
        "          predicted_probs = torch.sigmoid(logits)\n",
        "\n",
        "          predicted_labels.extend(predicted_probs.cpu().numpy())\n",
        "          p_test_ids.extend(test_ids[cnt])\n",
        "          cnt+=1\n",
        "\n",
        "\n",
        "  p_labels = predicted_labels\n",
        "  # Apply a threshold to predicted probabilities to determine the predicted labels\n",
        "  threshold = 0.25  # You can adjust this threshold based on your requirements\n",
        "  class_threshold = [0.4, 0.3, 0.2, 0.01, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1, 0.01, 0.01, 0.2, 0.1, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1]\n",
        "  predicted_labels = (np.array(predicted_labels) > threshold).astype(int)\n",
        "\n",
        "\n",
        "  # Prepare the results in the desired format (id and labels) and save to a JSON file\n",
        "  results = [{'id': test_id, 'labels': [all_labels[i] for i, value in enumerate(instance) if value == 1]} for test_id, instance in zip(test_ids, predicted_labels)]\n",
        "\n",
        "  output_file_path = \"path_to_test_results\" + \"_\" + model + \".json\"\n",
        "  with open(output_file_path, 'w') as output_file:\n",
        "      json.dump(results, output_file, indent=2)\n",
        "\n",
        "  print(f\"Predictions saved to {output_file_path}\")\n",
        "\n",
        "  return p_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "z9Z-YOrIJ-mJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DMXuix__mnt"
      },
      "source": [
        "##BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDxi3AkL_nQT"
      },
      "outputs": [],
      "source": [
        "# Initialize a tokenizer for your chosen pre-trained model\n",
        "model_name_1 = \"bert-base-uncased\"  # You can replace this with your preferred model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDUaai3G_oZG"
      },
      "outputs": [],
      "source": [
        "bert_model = AutoModel.from_pretrained(model_name_1)\n",
        "\n",
        "# Define the custom classifier model\n",
        "class CustomClassifierBERT(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(CustomClassifierBERT, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = bert_model.to(device)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token output\n",
        "        return logits\n",
        "\n",
        "# Instantiate the classifier model\n",
        "num_labels = len(all_labels)  # The number of labels in your dataset\n",
        "classifier_model_1 = CustomClassifierBERT(num_labels).to(device) # or you can load a saved model\n",
        "\n",
        "\n",
        "\n",
        "train_input_ids, train_attention_mask, train_labels, valid_input_ids, valid_attention_mask, valid_labels, test_input_ids, test_attention_mask, test_ids = prepare_data(train_data, valid_data, test_data, model_name_1)\n",
        "\n",
        "# Convert data to PyTorch DataLoader for training\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_attention_mask, valid_labels)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_mask)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))"
      ],
      "metadata": {
        "id": "v2WBpobd_tUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xvObdgZ_-TB"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "c_model_1 = train_model(classifier_model_1,train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDBza1C0ARLE"
      },
      "outputs": [],
      "source": [
        "# Validate the model\n",
        "c_model_1.eval()\n",
        "bert_val_labels, true_labels = validate_model(c_model_1, valid_dataloader,\"bert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSz9ANa5AmGG"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "c_model_1.eval()\n",
        "bert_labels = test_model(c_model_1, test_dataloader, \"bert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK5zCFdF91xx"
      },
      "outputs": [],
      "source": [
        "print(len(bert_labels))\n",
        "print(np.shape(bert_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(c_model_1.state_dict(), 'path_to_model.pth')"
      ],
      "metadata": {
        "id": "HoOjg7qiGwuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNT7lRo2BSd5"
      },
      "source": [
        "##RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzOoVbBiBWwz"
      },
      "outputs": [],
      "source": [
        "# Initialize a tokenizer for your chosen pre-trained model\n",
        "model_name_2 = \"xlm-roberta-base\"  # You can replace this with your preferred model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2bzyOIaBc7t"
      },
      "outputs": [],
      "source": [
        "#tokenizer_roberta = RobertaTokenizer.from_pretrained(model_name_2)\n",
        "roberta_model = AutoModel.from_pretrained(model_name_2)\n",
        "\n",
        "class CustomClassifierRoBERTa(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(CustomClassifierRoBERTa, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.roberta = roberta_model.to(device)\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
        "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Instantiate the classifier model\n",
        "num_labels = len(all_labels)  # The number of labels in your dataset\n",
        "classifier_model_2 = CustomClassifierRoBERTa(num_labels).to(device)\n",
        "\n",
        "train_input_ids, train_attention_mask, train_labels, valid_input_ids, valid_attention_mask, valid_labels, test_input_ids, test_attention_mask, test_ids = prepare_data(train_data, valid_data, test_data, model_name_2)\n",
        "\n",
        "# Convert data to PyTorch DataLoader for training\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_attention_mask, valid_labels)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_mask)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))"
      ],
      "metadata": {
        "id": "KsBaQQlrU0l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYf3kpmCBe4-"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "c_model_2 = train_model(classifier_model_2,train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW4wHu8zBg_p"
      },
      "outputs": [],
      "source": [
        "# Validate the model\n",
        "c_model_2.eval()\n",
        "roberta_val_labels, true_labels = validate_model(c_model_2, valid_dataloader,\"roberta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhqr3OR8Bjhs"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "c_model_2.eval()\n",
        "roberta_labels = test_model(c_model_2, test_dataloader, \"roberta\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(c_model_2.state_dict(), 'path_to_model.pth')"
      ],
      "metadata": {
        "id": "6Xl4IWiWPm4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TCsbOKUEVd4"
      },
      "source": [
        "## mBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdYR06NcEUpP"
      },
      "outputs": [],
      "source": [
        "# Initialize mbert tokenizer and model\n",
        "model_name_3 = \"bert-base-multilingual-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKQh10NFEpzV"
      },
      "outputs": [],
      "source": [
        "mbert_model = AutoModel.from_pretrained(model_name_3)\n",
        "\n",
        "# Define the custom classifier model\n",
        "class CustomClassifiermBERT(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(CustomClassifiermBERT, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.mbert = mbert_model.to(device)\n",
        "        self.classifier = nn.Linear(self.mbert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.mbert(input_ids, attention_mask=attention_mask)\n",
        "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token output\n",
        "        return logits\n",
        "\n",
        "# Instantiate the classifier model\n",
        "num_labels = len(all_labels)  # The number of labels in your dataset\n",
        "classifier_model_3 = CustomClassifiermBERT(num_labels).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_input_ids, train_attention_mask, train_labels, valid_input_ids, valid_attention_mask, valid_labels, test_input_ids, test_attention_mask, test_ids = prepare_data(train_data, valid_data, test_data, model_name_3)\n",
        "\n",
        "# Convert data to PyTorch DataLoader for training\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_attention_mask, valid_labels)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_mask)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqarHW1SEyjm"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "c_model_3 = train_model(classifier_model_3,train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDcRqY9pFLg3"
      },
      "outputs": [],
      "source": [
        "# Validate the model\n",
        "c_model_3.eval()\n",
        "mbert_val_labels, true_labels = validate_model(c_model_3, valid_dataloader,\"mbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLTcTLmBFPe6"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "c_model_3.eval()\n",
        "m_bert_labels = test_model(c_model_3, test_dataloader, \"mbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIQHwXfwdS2J"
      },
      "outputs": [],
      "source": [
        "torch.save(c_model_3.state_dict(), 'path_to_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble-Validation Data"
      ],
      "metadata": {
        "id": "bIjjGTnACA5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "AdgYNdkKfKPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_h_metrics(bert_val_labels, roberta_val_labels, mbert_val_labels,model):\n",
        "\n",
        "  predicted_labels_model1 = np.array(bert_val_labels)\n",
        "  predicted_labels_model2 = np.array(roberta_val_labels)\n",
        "  predicted_labels_model3 = np.array(mbert_val_labels)\n",
        "\n",
        "  ensemble_predictions = (predicted_labels_model1 + predicted_labels_model2 + predicted_labels_model3) / 3\n",
        "\n",
        "  # Apply a threshold to determine the final predicted labels\n",
        "  threshold = 0.5  # You can adjust this threshold based on your requirements\n",
        "  class_threshold = [0.4, 0.3, 0.2, 0.01, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1, 0.01, 0.01, 0.2, 0.1, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1]\n",
        "  final_predicted_labels = (ensemble_predictions > class_threshold).astype(int)\n",
        "\n",
        "  # Prepare the results in the desired format (id and labels) and save to a JSON file\n",
        "  results = [{'id': valid_id, 'labels': [all_labels[i] for i, value in enumerate(instance) if value == 1]} for valid_id, instance in zip(valid_ids, final_predicted_labels)]\n",
        "\n",
        "  output_file_path = \"path_to_val_results\" + model + \"_.json\"\n",
        "  with open(output_file_path, 'w') as output_file:\n",
        "      json.dump(results, output_file, indent=2)\n",
        "\n",
        "  precision_h, recall_h, f1_h = calculate_metrics(G, output_file_path)\n",
        "  print(\"HP\",precision_h)\n",
        "  print(\"HR\",recall_h)\n",
        "  print(\"HF\",f1_h)\n"
      ],
      "metadata": {
        "id": "BvXL_DIdzAoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_h_metrics(bert_val_labels, roberta_val_labels, mbert_val_labels,\"ensemble\")"
      ],
      "metadata": {
        "id": "QrKuG0Oyz7ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics_ensemble_val(bert_val_labels, roberta_val_labels, mbert_val_labels, true_labels, save_path, save_path_metrics):\n",
        "\n",
        "  predicted_labels_model1 = np.array(bert_val_labels)\n",
        "  predicted_labels_model2 = np.array(roberta_val_labels)\n",
        "  predicted_labels_model3 = np.array(mbert_val_labels)\n",
        "\n",
        "\n",
        "  # Combine the predictions by element-wise averaging\n",
        "  ensemble_predictions = (predicted_labels_model1 + predicted_labels_model2 + predicted_labels_model3) / 3\n",
        "\n",
        "  # Apply a threshold to determine the final predicted labels\n",
        "  threshold = 0.5  # You can adjust this threshold based on your requirements\n",
        "  class_threshold = [0.4, 0.3, 0.2, 0.01, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1, 0.01, 0.01, 0.2, 0.1, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1]\n",
        "  precision, recall, f1, support = precision_recall_fscore_support(true_labels, final_predicted_labels, average=None, zero_division=0)\n",
        "\n",
        "  # Create a list to store the results\n",
        "  results = []\n",
        "\n",
        "  # Populate the results list with class-wise metrics\n",
        "  for i, class_name in enumerate(all_labels):\n",
        "    results.append([class_name, f\"{f1[i]:.2f}\", f\"{precision[i]:.2f}\", f\"{recall[i]:.2f}\"])\n",
        "\n",
        "  # Print the results in a table format\n",
        "  headers = [\"Class\", \"F1-Score\", \"Precision\", \"Recall\"]\n",
        "  table = tabulate(results, headers=headers, tablefmt=\"grid\")\n",
        "  print(tabulate(results, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "  #Save the table in human pretty format\n",
        "\n",
        "  with open(save_path, 'w') as file:\n",
        "    file.write(table)\n",
        "  print(f\"Table saved to {save_path}\")\n",
        "\n",
        "\n",
        "  # Create a DataFrame using pandas\n",
        "  df = pd.DataFrame(results, columns=headers)\n",
        "\n",
        "  # Save the DataFrame to a CSV file\n",
        "  df.to_csv(save_path_metrics, index=False)\n",
        "  print(f\"Table saved to {save_path_metrics}\")\n"
      ],
      "metadata": {
        "id": "sOeCi0o4B_WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_metrics_ensemble_val(bert_val_labels, roberta_val_labels, mbert_val_labels, true_labels,\n",
        "                               'file_1.csv',\n",
        "                               'file_2.csv')"
      ],
      "metadata": {
        "id": "tZAT2OhhfcnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble-Test Data"
      ],
      "metadata": {
        "id": "mRehKb-NTbr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test(bert_labels, roberta_labels, m_bert_labels, model):\n",
        "\n",
        "  predicted_labels_model1 = np.array(bert_labels)\n",
        "  predicted_labels_model2 = np.array(roberta_labels)\n",
        "  predicted_labels_model3 = np.array(m_bert_labels)\n",
        "\n",
        "  # Combine the predictions by element-wise averaging\n",
        "  ensemble_predictions = (predicted_labels_model1 + predicted_labels_model2 + predicted_labels_model3) / 3\n",
        "\n",
        "  # Apply a threshold to determine the final predicted labels\n",
        "  threshold = 0.5  # You can adjust this threshold based on your requirements\n",
        "  class_threshold = [0.4, 0.3, 0.2, 0.01, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1, 0.01, 0.01, 0.2, 0.1, 0.7, 0.5, 0.4, 0.4, 0.2, 0.1]\n",
        "  final_predicted_labels = (ensemble_predictions > class_threshold).astype(int)\n",
        "\n",
        "\n",
        "  # Prepare the results in the desired format (id and labels) and save to a JSON file\n",
        "  results = [{'id': test_id, 'labels': [all_labels[i] for i, value in enumerate(instance) if value == 1]} for test_id, instance in zip(test_ids, final_predicted_labels)]\n",
        "\n",
        "  output_file_path = \"path_to_test_results\" + model +  \"_test_ensemble.json\"\n",
        "  with open(output_file_path, 'w',encoding='utf-8') as output_file:\n",
        "    json.dump(results, output_file, ensure_ascii = False, indent=2)\n",
        "\n",
        "  print(f\"Predictions saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "HZb5SouzTYFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_test(bert_labels, roberta_labels, m_bert_labels, \"ensemble\")"
      ],
      "metadata": {
        "id": "3f2TgjWfTaSk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}